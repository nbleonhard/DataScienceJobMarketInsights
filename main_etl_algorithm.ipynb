{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ETL Algorithm\n",
    "The objective of this notebook is to autonomously scrape job listings from google, transform the data, and upload it into a PostgreSQL server. \n",
    "\n",
    "The algorithm searches and scrapes job listings for 5 data oriented job roles in 14 major cities in the United States. \n",
    "\n",
    "Some information such as API keys, host names, and database names have been omitted for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results # installs the required google-search-results package into environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import getpass\n",
    "from datetime import date\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_data(job, search_term):\n",
    "    '''\n",
    "    This function collects all the desired data from a single job posting.\n",
    "    A job posting is a dictionary that contains several other lists and dictionaries.\n",
    "    This algorithm navigates through these data structures and collects the \n",
    "    desired data. If the data is not available than the field is filled with np.nan.\n",
    "    This data is then packaged into a dictionary and returned to the caller.\n",
    "    \n",
    "    Args:\n",
    "        job : Dict\n",
    "            A single job posting returned by a API search\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "    \n",
    "    Returns:\n",
    "        Dict\n",
    "            A dictionary containing the desired features and data from the job posting.\n",
    "\n",
    "    '''\n",
    "    search_term = search_term\n",
    "    title = job['title']\n",
    "    company_name = job['company_name']\n",
    "#     location = job['location']\n",
    "    description = job['description']\n",
    "    job_id = job['job_id']\n",
    "    \n",
    "    # A job postings can not have a location so we need to account for this\n",
    "    if 'location' not in job:\n",
    "        location = np.nan\n",
    "    else:\n",
    "        location = job['location']\n",
    "\n",
    "    '''\n",
    "    The outer if statement is necessary because some jobs have a single highlight with no title key.\n",
    "    I suspect this happens when no highlights are provided by the employer.\n",
    "    For these jobs, there's no text to assign to the qualifications, responsibilities, or benefits variables.\n",
    "    When this happens, I'll assign the text to a variable called 'items'\n",
    "    It's likely that this text is the same as the description text but I'll store it just in case.\n",
    "    \n",
    "    Note: I'm making some assumptions here. \n",
    "    It's possible a job has no highlights in the job_highlights list.\n",
    "    It's possible a job with multiple highlights will have a highlight with no title key.\n",
    "    Here are the only permuations I'm considering based on my analysis of the data:\n",
    "        - [{'items': ['']}]\n",
    "        - [{'title': '', 'items': ['']}]\n",
    "        - [{'title': '', 'items': ['']}, {'title': '', 'items': ['']}, ... ]\n",
    "    '''\n",
    "    \n",
    "    # If the first highlight has no 'title' key\n",
    "    if 'title' not in job['job_highlights'][0]:\n",
    "        items = ''\n",
    "        for element in job['job_highlights'][0]['items']:\n",
    "            items += element + '\\n'\n",
    "        qualifications = np.nan\n",
    "        responsibilities = np.nan\n",
    "        benefits = np.nan\n",
    "    \n",
    "    else:\n",
    "        items = np.nan\n",
    "        qualifications = np.nan\n",
    "        responsibilities = np.nan\n",
    "        benefits = np.nan\n",
    "        \n",
    "        for highlight in job['job_highlights']:\n",
    "            if 'title' and 'items' in highlight: # can probably remove this if-statement\n",
    "                if highlight['title'] == 'Qualifications':\n",
    "                    qualifications = ''\n",
    "                    for element in highlight['items']:\n",
    "                        qualifications += element + '\\n'\n",
    "                if highlight['title'] == 'Responsibilities':\n",
    "                    responsibilities = ''\n",
    "                    for element in highlight['items']:\n",
    "                        responsibilities += element + '\\n'\n",
    "                if highlight['title'] == 'Benefits':\n",
    "                    benefits = ''\n",
    "                    for element in highlight['items']:\n",
    "                        benefits += element + '\\n'\n",
    "    \n",
    "    date_scraped = pd.to_datetime(str(date.today())) # aka the current date\n",
    "    date_posted = np.nan\n",
    "    posted_at = np.nan\n",
    "    schedule_type = np.nan\n",
    "    work_from_home = np.nan\n",
    "    salary = np.nan\n",
    "    \n",
    "    for extension in job['detected_extensions']:\n",
    "        if extension == 'posted_at':\n",
    "            posted_at = job['detected_extensions'][extension]\n",
    "            date_posted = date_scraped - pd.Timedelta(days=int(posted_at.split()[0]))\n",
    "        if extension == 'schedule_type':\n",
    "            schedule_type = job['detected_extensions'][extension]\n",
    "        if extension == 'work_from_home':\n",
    "            work_from_home = bool(job['detected_extensions'][extension])\n",
    "        if extension == 'salary':\n",
    "            salary = job['detected_extensions'][extension]\n",
    "    \n",
    "    # ommit 'via ' from 'via LinkedIn' before assignment\n",
    "    via = job['via'][3:]\n",
    "    \n",
    "    return {\n",
    "        'search_term': search_term,\n",
    "        'title': title,\n",
    "        'company_name': company_name,\n",
    "        'location': location,\n",
    "        'description': description,\n",
    "        'job_id': job_id,\n",
    "        'qualifications': qualifications, \n",
    "        'responsibilities': responsibilities, \n",
    "        'benefits': benefits, \n",
    "        'items': items,\n",
    "        'via': via,\n",
    "        'posted_at': posted_at,\n",
    "        'schedule_type': schedule_type,\n",
    "        'work_from_home': work_from_home,\n",
    "        'salary': salary,\n",
    "        'date_posted': date_posted,\n",
    "        'date_scraped': date_scraped\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_jobs_from_api(search_term, location, start):\n",
    "    '''\n",
    "    This function connects and sends a request to the SerpAPI. SerpAPI searches the job postings\n",
    "    on google with the given search parameters, scrapes the data, and then responds with the results.\n",
    "    Each search can return a maximum of ten job posting.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "        start: Int\n",
    "            Pagination. Tells the API what page we want to pull job postings from.\n",
    "    Returns:\n",
    "        List\n",
    "            Returns a list of lists. \n",
    "            The list containing the job postings is in the 'jobs_results' key.\n",
    "            IMPORTANT: if a search results in no job postings, there will be no\n",
    "            'jobs_results' key and instead there will be a 'error' key\n",
    "    '''\n",
    "    \n",
    "    params = {\n",
    "    \"api_key\": \"my_api_key\",\n",
    "    \"engine\": \"google_jobs\",\n",
    "    \"google_domain\": \"google.com\",\n",
    "    \"q\": search_term, \n",
    "    \"hl\": \"en\",\n",
    "    \"gl\": \"us\",\n",
    "    \"location\": location,\n",
    "    \"start\": start\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs(search_term, location):\n",
    "    '''\n",
    "    This function collects all the job postings for an entire city/location.\n",
    "    All job postings are stored into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame()\n",
    "            Contains all job postings pulled for an entire city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    print('Scraping all {} job postings in {}'.format(search_term, location))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    error = False\n",
    "    start = 0\n",
    "    \n",
    "    while(error == False): \n",
    "        results = pull_jobs_from_api(search_term, location, start)\n",
    "        if 'error' in results:\n",
    "            error = True\n",
    "        else:\n",
    "            for job in results['jobs_results']:\n",
    "                df = df.append(get_job_data(job, search_term), ignore_index=True)\n",
    "        print('Page {} complete.'.format(int(start / 10) + 1))\n",
    "        start += 10\n",
    "    \n",
    "    print('Done.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will probably be depricated\n",
    "def get_n_jobs(search_term, location, end):\n",
    "    '''\n",
    "    This function collects n job postings for a city/location.\n",
    "    The job postings are stored into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "        end: Int\n",
    "            The page you want to stop searching.\n",
    "            start=0 -> first page of results\n",
    "            start=10 -> second page of results\n",
    "            start=20 -> third page of results\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains n job postings pulled for a city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    print('Scraping all {} job postings in {}'.format(search_term, location))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    error = False\n",
    "    start = 0\n",
    "    end = end\n",
    "    \n",
    "    while(error == False and start <= end): \n",
    "        results = pull_ten_jobs(search_term, location, start)\n",
    "        if 'error' in results:\n",
    "            error = True\n",
    "        else:\n",
    "            for job in results['jobs_results']:\n",
    "                df = df.append(get_job_data(job, search_term), ignore_index=True)\n",
    "        start += 10\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_df_to_db(df, schema, table, username, password):\n",
    "    '''\n",
    "    This function takes the dataframe containing job posting data for\n",
    "    a city and SQL injects it into the defined schema and table.\n",
    "    In my case, I am uploading to a PostgreSQL server at my University.\n",
    "    \n",
    "    Args:\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing job posting data for a city\n",
    "        schema : Str\n",
    "        table : Str\n",
    "        username : Str\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    password = password\n",
    "    username = username\n",
    "    host = 'host_name'\n",
    "    database = 'database_name'\n",
    "    \n",
    "    # first, check for duplicates\n",
    "    df = remove_duplicates(df, username, password, schema, table)\n",
    "    \n",
    "    postgres_db = {\n",
    "        'drivername': 'postgres',\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'host': host,\n",
    "        'database': database\n",
    "    }\n",
    "    \n",
    "    engine = create_engine(URL(**postgres_db), echo=False)\n",
    "    \n",
    "    df.to_sql(\n",
    "        table, \n",
    "        engine, \n",
    "        schema = schema, \n",
    "        if_exists=\"append\", \n",
    "        index=False\n",
    "    )\n",
    "    print('{} jobs pushed to {}.{}'.format(df.shape[0], schema, table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(sql_query, user):\n",
    "    '''\n",
    "    This function take a SQL query and queries the PostgreSQL database.\n",
    "    It returns a dataframe containing the result of that query.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: Str\n",
    "            Contains the SQL query you want to query the database with.\n",
    "        user: Str\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains result of your query.\n",
    "    '''\n",
    "    \n",
    "    sql_query = sql_query\n",
    "    database = \"database_name\"\n",
    "    user     = user\n",
    "    password = getpass.getpass(\"Enter password: \")\n",
    "\n",
    "    connection = psycopg2.connect(\n",
    "        database = database,\n",
    "        user = user,\n",
    "        host = 'host_name',\n",
    "        password = password)\n",
    "    \n",
    "    df = pd.read_sql_query(sql_query, connection)\n",
    "    connection.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs_for_all_search_terms(search_terms, location):\n",
    "    '''\n",
    "    Args:\n",
    "        search_terms : List\n",
    "            Contains search terms used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all job postings for all listed search terms pulled for an entire city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for search_term in search_terms:\n",
    "        df = pd.concat([df, get_all_jobs(search_term, location)], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, user, password, schema, table):\n",
    "    '''\n",
    "    This function removes any job postings with a job_id that is already in the database.\n",
    "    This is to prevent any rows from violating the primary key constraints which would \n",
    "    reject the entire insert.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the job posting data you want to upload to the database.\n",
    "        user: Str\n",
    "        password: Str\n",
    "        schema: Str\n",
    "        table: Str\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all unique jobs to be uploaded to the database.\n",
    "    '''\n",
    "    \n",
    "    primary_keys = get_db_primary_keys(user, password, schema, table)['job_id'].tolist()\n",
    "    jobs_removed = 0\n",
    "    \n",
    "    for job_id in df['job_id']:\n",
    "        if job_id in primary_keys:\n",
    "            df = df[df['job_id'] != job_id] # remove the job/row from the dataframe\n",
    "            jobs_removed += 1\n",
    "    print('{} duplicate jobs found and removed.'.format(jobs_removed))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_primary_keys(user, password, schema, table):\n",
    "    '''\n",
    "    This function querries is the database for all primary keys in a given schema and table.\n",
    "    \n",
    "    Args:\n",
    "        user: Str\n",
    "        password: Str\n",
    "        schema: Str\n",
    "        table: Str\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all job_ids/primary keys for the given schema and table.\n",
    "    '''\n",
    "    sql_query = 'SELECT job_id FROM {}.{}'.format(schema, table)\n",
    "    database = \"database_name\"\n",
    "    user     = user\n",
    "    password = password\n",
    "\n",
    "    connection = psycopg2.connect(\n",
    "        database = database,\n",
    "        user = user,\n",
    "        host = 'host_name',\n",
    "        password = password)\n",
    "    \n",
    "    df = pd.read_sql_query(sql_query, connection)\n",
    "    connection.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_etl(search_terms, locations, schema, table, username):\n",
    "    '''\n",
    "    This function collects all the available job listings for all defined job roles and uploads \n",
    "    them to the PostgreSQL server. This is then repeated for all defined locations. \n",
    "    \n",
    "    Args:\n",
    "        search_terms: List\n",
    "            Contains a list of job roles to search for\n",
    "        locations: List\n",
    "            Contains a list of locations \n",
    "        schema: Str\n",
    "        table: Str\n",
    "        username: Str\n",
    "        password: Str\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    '''\n",
    "    \n",
    "    search_terms = search_terms\n",
    "    locations = locations\n",
    "    schema = schema\n",
    "    table = table\n",
    "    username = username\n",
    "    password = getpass.getpass('Enter password: ')\n",
    "    \n",
    "    for location in locations:\n",
    "        data = get_all_jobs_for_all_search_terms(search_terms, location)\n",
    "        print('{} jobs collected from {}'.format(data.shape[0], location))\n",
    "        push_df_to_db(data, schema, table, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\n",
    "    'New York, New York, United States', # 23.6 M\n",
    "    'Los Angeles, CA, California, United States', # 22.9 M\n",
    "    'Chicago, Illinois, United States', # 21.4 M\n",
    "    'San Francisco Bay Area, United States', # 18.9 M\n",
    "    'Houston, TX, Texas, United States', # 13.1 M\n",
    "    'Miami, Florida, United States', # 8.4 M\n",
    "    'Boston, Massachusetts, United States', # 8.1 M\n",
    "    'Phoenix, AZ, Arizona, United States', # 6.2 M\n",
    "    'Philadelphia, Pennsylvania, United States', # 6.1 M\n",
    "    'Austin, TX, Texas, United States', # 5.5 M\n",
    "    'Kansas City, Missouri, United States', # 2.1 M\n",
    "    'Seattle, Washington, United States', # 5.4 M\n",
    "    'Washington, District of Columbia, United States', # 11.4 M\n",
    "    'Denver, CO, Colorado, United States' # 6.1 M\n",
    "]\n",
    "\n",
    "search_terms = [\n",
    "    'Data Scientist',\n",
    "    'Data Analyst', \n",
    "    'Data Engineer',\n",
    "    'Machine Learning Engineer',\n",
    "    'Business Intelligence Analyst'\n",
    "]\n",
    "\n",
    "schema = 'schema_name'\n",
    "table = 'table_name'\n",
    "username = 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main\n",
    "'''\n",
    "batch_etl(search_terms, locations, schema, table, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = 'SELECT * FROM {}.{}'.format(schema, table)\n",
    "test = query_database(sql_query, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
