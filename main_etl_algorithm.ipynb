{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ETL Algorithm\n",
    "The objective of this notebook is to autonomously scrape job listings from google, transform the data, and upload it into a PostgreSQL server. \n",
    "\n",
    "The algorithm searches and scrapes job listings for 5 data oriented job roles in 14 major cities in the United States. \n",
    "\n",
    "Some information such as API keys, host names, and database names have been omitted for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading https://files.pythonhosted.org/packages/77/30/b3a6f6a2e00f8153549c2fa345c58ae1ce8e5f3153c2fe0484d444c3abcb/google_search_results-2.4.2.tar.gz\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from google-search-results) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->google-search-results) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->google-search-results) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->google-search-results) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->google-search-results) (2019.9.11)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-cp37-none-any.whl size=32004 sha256=8cba5b712005ca5ae6a2778e6639aa4e1cd561d018101a0b284b5f7078a5b4a0\n",
      "  Stored in directory: /home/nblkd6/.cache/pip/wheels/8e/86/9b/b4debab19a41bbfac16c7149b17ce81e63234914bd638a1a0e\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results # installs the required google-search-results package into environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import getpass\n",
    "from datetime import date\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_data(job, search_term):\n",
    "    '''\n",
    "    This function collects all the desired data from a single job posting.\n",
    "    A job posting is a dictionary that contains several other lists and dictionaries.\n",
    "    This algorithm navigates through these data structures and collects the \n",
    "    desired data. If the data is not available than the field is filled with np.nan.\n",
    "    This data is then packaged into a dictionary and returned to the caller.\n",
    "    \n",
    "    Args:\n",
    "        job : Dict\n",
    "            A single job posting returned by a API search\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "    \n",
    "    Returns:\n",
    "        Dict\n",
    "            A dictionary containing the desired features and data from the job posting.\n",
    "\n",
    "    '''\n",
    "    search_term = search_term\n",
    "    title = job['title']\n",
    "    company_name = job['company_name']\n",
    "#     location = job['location']\n",
    "    description = job['description']\n",
    "    job_id = job['job_id']\n",
    "    \n",
    "    # A job postings can not have a location so we need to account for this\n",
    "    if 'location' not in job:\n",
    "        location = np.nan\n",
    "    else:\n",
    "        location = job['location']\n",
    "\n",
    "    '''\n",
    "    The outer if statement is necessary because some jobs have a single highlight with no title key.\n",
    "    I suspect this happens when no highlights are provided by the employer.\n",
    "    For these jobs, there's no text to assign to the qualifications, responsibilities, or benefits variables.\n",
    "    When this happens, I'll assign the text to a variable called 'items'\n",
    "    It's likely that this text is the same as the description text but I'll store it just in case.\n",
    "    \n",
    "    Note: I'm making some assumptions here. \n",
    "    It's possible a job has no highlights in the job_highlights list.\n",
    "    It's possible a job with multiple highlights will have a highlight with no title key.\n",
    "    Here are the only permuations I'm considering based on my analysis of the data:\n",
    "        - [{'items': ['']}]\n",
    "        - [{'title': '', 'items': ['']}]\n",
    "        - [{'title': '', 'items': ['']}, {'title': '', 'items': ['']}, ... ]\n",
    "    '''\n",
    "    \n",
    "    # If the first highlight has no 'title' key\n",
    "    if 'title' not in job['job_highlights'][0]:\n",
    "        items = ''\n",
    "        for element in job['job_highlights'][0]['items']:\n",
    "            items += element + '\\n'\n",
    "        qualifications = np.nan\n",
    "        responsibilities = np.nan\n",
    "        benefits = np.nan\n",
    "    \n",
    "    else:\n",
    "        items = np.nan\n",
    "        qualifications = np.nan\n",
    "        responsibilities = np.nan\n",
    "        benefits = np.nan\n",
    "        \n",
    "        for highlight in job['job_highlights']:\n",
    "            if 'title' and 'items' in highlight: # can probably remove this if-statement\n",
    "                if highlight['title'] == 'Qualifications':\n",
    "                    qualifications = ''\n",
    "                    for element in highlight['items']:\n",
    "                        qualifications += element + '\\n'\n",
    "                if highlight['title'] == 'Responsibilities':\n",
    "                    responsibilities = ''\n",
    "                    for element in highlight['items']:\n",
    "                        responsibilities += element + '\\n'\n",
    "                if highlight['title'] == 'Benefits':\n",
    "                    benefits = ''\n",
    "                    for element in highlight['items']:\n",
    "                        benefits += element + '\\n'\n",
    "    \n",
    "    date_scraped = pd.to_datetime(str(date.today())) # aka the current date\n",
    "    date_posted = np.nan\n",
    "    posted_at = np.nan\n",
    "    schedule_type = np.nan\n",
    "    work_from_home = np.nan\n",
    "    salary = np.nan\n",
    "    \n",
    "    for extension in job['detected_extensions']:\n",
    "        if extension == 'posted_at':\n",
    "            posted_at = job['detected_extensions'][extension]\n",
    "            date_posted = date_scraped - pd.Timedelta(days=int(posted_at.split()[0]))\n",
    "        if extension == 'schedule_type':\n",
    "            schedule_type = job['detected_extensions'][extension]\n",
    "        if extension == 'work_from_home':\n",
    "            work_from_home = bool(job['detected_extensions'][extension])\n",
    "        if extension == 'salary':\n",
    "            salary = job['detected_extensions'][extension]\n",
    "    \n",
    "    # ommit 'via ' from 'via LinkedIn' before assignment\n",
    "    via = job['via'][3:]\n",
    "    \n",
    "    return {\n",
    "        'search_term': search_term,\n",
    "        'title': title,\n",
    "        'company_name': company_name,\n",
    "        'location': location,\n",
    "        'description': description,\n",
    "        'job_id': job_id,\n",
    "        'qualifications': qualifications, \n",
    "        'responsibilities': responsibilities, \n",
    "        'benefits': benefits, \n",
    "        'items': items,\n",
    "        'via': via,\n",
    "        'posted_at': posted_at,\n",
    "        'schedule_type': schedule_type,\n",
    "        'work_from_home': work_from_home,\n",
    "        'salary': salary,\n",
    "        'date_posted': date_posted,\n",
    "        'date_scraped': date_scraped\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_jobs_from_api(search_term, location, start):\n",
    "    '''\n",
    "    This function connects and sends a request to the SerpAPI. SerpAPI searches the job postings\n",
    "    on google with the given search parameters, scrapes the data, and then responds with the results.\n",
    "    Each search can return a maximum of ten job posting.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "        start: Int\n",
    "            Pagination. Tells the API what page we want to pull job postings from.\n",
    "    Returns:\n",
    "        List\n",
    "            Returns a list of lists. \n",
    "            The list containing the job postings is in the 'jobs_results' key.\n",
    "            IMPORTANT: if a search results in no job postings, there will be no\n",
    "            'jobs_results' key and instead there will be a 'error' key\n",
    "    '''\n",
    "    \n",
    "    params = {\n",
    "    \"api_key\": \"my_api_key\",\n",
    "    \"engine\": \"google_jobs\",\n",
    "    \"google_domain\": \"google.com\",\n",
    "    \"q\": search_term, \n",
    "    \"hl\": \"en\",\n",
    "    \"gl\": \"us\",\n",
    "    \"location\": location,\n",
    "    \"start\": start\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs(search_term, location):\n",
    "    '''\n",
    "    This function collects all the job postings for an entire city/location.\n",
    "    All job postings are stored into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame()\n",
    "            Contains all job postings pulled for an entire city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    print('Scraping all {} job postings in {}'.format(search_term, location))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    error = False\n",
    "    start = 0\n",
    "    \n",
    "    while(error == False): \n",
    "        results = pull_jobs_from_api(search_term, location, start)\n",
    "        if 'error' in results:\n",
    "            error = True\n",
    "        else:\n",
    "            for job in results['jobs_results']:\n",
    "                df = df.append(get_job_data(job, search_term), ignore_index=True)\n",
    "        print('Page {} complete.'.format(int(start / 10) + 1))\n",
    "        start += 10\n",
    "    \n",
    "    print('Done.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will probably be depricated\n",
    "def get_n_jobs(search_term, location, end):\n",
    "    '''\n",
    "    This function collects n job postings for a city/location.\n",
    "    The job postings are stored into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        search_term : Str\n",
    "            The search term used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "        end: Int\n",
    "            The page you want to stop searching.\n",
    "            start=0 -> first page of results\n",
    "            start=10 -> second page of results\n",
    "            start=20 -> third page of results\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains n job postings pulled for a city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    print('Scraping all {} job postings in {}'.format(search_term, location))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    error = False\n",
    "    start = 0\n",
    "    end = end\n",
    "    \n",
    "    while(error == False and start <= end): \n",
    "        results = pull_ten_jobs(search_term, location, start)\n",
    "        if 'error' in results:\n",
    "            error = True\n",
    "        else:\n",
    "            for job in results['jobs_results']:\n",
    "                df = df.append(get_job_data(job, search_term), ignore_index=True)\n",
    "        start += 10\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_df_to_db(df, schema, table, username, password):\n",
    "    '''\n",
    "    This function takes the dataframe containing job posting data for\n",
    "    a city and SQL injects it into the defined schema and table.\n",
    "    In my case, I am uploading to a PostgreSQL server at my University.\n",
    "    \n",
    "    Args:\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing job posting data for a city\n",
    "        schema : Str\n",
    "        table : Str\n",
    "        username : Str\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    password = password\n",
    "    username = username\n",
    "    host = 'host_name'\n",
    "    database = 'database_name'\n",
    "    \n",
    "    # first, check for duplicates\n",
    "    df = remove_duplicates(df, username, password, schema, table)\n",
    "    \n",
    "    postgres_db = {\n",
    "        'drivername': 'postgres',\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'host': host,\n",
    "        'database': database\n",
    "    }\n",
    "    \n",
    "    engine = create_engine(URL(**postgres_db), echo=False)\n",
    "    \n",
    "    df.to_sql(\n",
    "        table, \n",
    "        engine, \n",
    "        schema = schema, \n",
    "        if_exists=\"append\", \n",
    "        index=False\n",
    "    )\n",
    "    print('{} jobs pushed to {}.{}'.format(df.shape[0], schema, table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(sql_query, user):\n",
    "    '''\n",
    "    This function take a SQL query and queries the PostgreSQL database.\n",
    "    It returns a dataframe containing the result of that query.\n",
    "    \n",
    "    Args:\n",
    "        sql_query: Str\n",
    "            Contains the SQL query you want to query the database with.\n",
    "        user: Str\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains result of your query.\n",
    "    '''\n",
    "    \n",
    "    sql_query = sql_query\n",
    "    database = \"database_name\"\n",
    "    user     = user\n",
    "    password = getpass.getpass(\"Enter password: \")\n",
    "\n",
    "    connection = psycopg2.connect(\n",
    "        database = database,\n",
    "        user = user,\n",
    "        host = 'host_name',\n",
    "        password = password)\n",
    "    \n",
    "    df = pd.read_sql_query(sql_query, connection)\n",
    "    connection.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs_for_all_search_terms(search_terms, location):\n",
    "    '''\n",
    "    Args:\n",
    "        search_terms : List\n",
    "            Contains search terms used to search for job postings on google\n",
    "        location: Str\n",
    "            The US city containing the job postings we want\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all job postings for all listed search terms pulled for an entire city.\n",
    "            Each row represents a single job posting.\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for search_term in search_terms:\n",
    "        df = pd.concat([df, get_all_jobs(search_term, location)], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, user, password, schema, table):\n",
    "    '''\n",
    "    This function removes any job postings with a job_id that is already in the database.\n",
    "    This is to prevent any rows from violating the primary key constraints which would \n",
    "    reject the entire insert.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the job posting data you want to upload to the database.\n",
    "        user: Str\n",
    "        password: Str\n",
    "        schema: Str\n",
    "        table: Str\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all unique jobs to be uploaded to the database.\n",
    "    '''\n",
    "    \n",
    "    primary_keys = get_db_primary_keys(user, password, schema, table)['job_id'].tolist()\n",
    "    jobs_removed = 0\n",
    "    \n",
    "    for job_id in df['job_id']:\n",
    "        if job_id in primary_keys:\n",
    "            df = df[df['job_id'] != job_id] # remove the job/row from the dataframe\n",
    "            jobs_removed += 1\n",
    "    print('{} duplicate jobs found and removed.'.format(jobs_removed))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_primary_keys(user, password, schema, table):\n",
    "    '''\n",
    "    This function querries is the database for all primary keys in a given schema and table.\n",
    "    \n",
    "    Args:\n",
    "        user: Str\n",
    "        password: Str\n",
    "        schema: Str\n",
    "        table: Str\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            Contains all job_ids/primary keys for the given schema and table.\n",
    "    '''\n",
    "    sql_query = 'SELECT job_id FROM {}.{}'.format(schema, table)\n",
    "    database = \"database_name\"\n",
    "    user     = user\n",
    "    password = password\n",
    "\n",
    "    connection = psycopg2.connect(\n",
    "        database = database,\n",
    "        user = user,\n",
    "        host = 'host_name',\n",
    "        password = password)\n",
    "    \n",
    "    df = pd.read_sql_query(sql_query, connection)\n",
    "    connection.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_etl(search_terms, locations, schema, table, username):\n",
    "    '''\n",
    "    This function collects all the available job listings for all defined job roles and uploads \n",
    "    them to the PostgreSQL server. This is then repeated for all defined locations. \n",
    "    \n",
    "    Args:\n",
    "        search_terms: List\n",
    "            Contains a list of job roles to search for\n",
    "        locations: List\n",
    "            Contains a list of locations \n",
    "        schema: Str\n",
    "        table: Str\n",
    "        username: Str\n",
    "        password: Str\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    '''\n",
    "    \n",
    "    search_terms = search_terms\n",
    "    locations = locations\n",
    "    schema = schema\n",
    "    table = table\n",
    "    username = username\n",
    "    password = getpass.getpass('Enter password: ')\n",
    "    \n",
    "    for location in locations:\n",
    "        data = get_all_jobs_for_all_search_terms(search_terms, location)\n",
    "        print('{} jobs collected from {}'.format(data.shape[0], location))\n",
    "        push_df_to_db(data, schema, table, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\n",
    "    'New York, New York, United States', # 23.6 M\n",
    "    'Los Angeles, CA, California, United States', # 22.9 M\n",
    "    'Chicago, Illinois, United States', # 21.4 M\n",
    "    'San Francisco Bay Area, United States', # 18.9 M\n",
    "    'Houston, TX, Texas, United States', # 13.1 M\n",
    "    'Miami, Florida, United States', # 8.4 M\n",
    "    'Boston, Massachusetts, United States', # 8.1 M\n",
    "    'Phoenix, AZ, Arizona, United States', # 6.2 M\n",
    "    'Philadelphia, Pennsylvania, United States', # 6.1 M\n",
    "    'Austin, TX, Texas, United States', # 5.5 M\n",
    "    'Kansas City, Missouri, United States', # 2.1 M\n",
    "    'Seattle, Washington, United States', # 5.4 M\n",
    "    'Washington, District of Columbia, United States', # 11.4 M\n",
    "    'Denver, CO, Colorado, United States' # 6.1 M\n",
    "]\n",
    "\n",
    "search_terms = [\n",
    "    'Data Scientist',\n",
    "    'Data Analyst', \n",
    "    'Data Engineer',\n",
    "    'Machine Learning Engineer',\n",
    "    'Business Intelligence Analyst'\n",
    "]\n",
    "\n",
    "schema = 'schema_name'\n",
    "table = 'table_name'\n",
    "username = 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter password: ········\n",
      "Scraping all Data Scientist job postings in Austin, TX, Texas, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Done.\n",
      "Scraping all Data Analyst job postings in Austin, TX, Texas, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Done.\n",
      "Scraping all Data Engineer job postings in Austin, TX, Texas, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Done.\n",
      "Scraping all Machine Learning Engineer job postings in Austin, TX, Texas, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Business Intelligence Analyst job postings in Austin, TX, Texas, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Done.\n",
      "1230 jobs collected from Austin, TX, Texas, United States\n",
      "0 duplicate jobs found and removed.\n",
      "1230 jobs pushed to raw.jobs\n",
      "Scraping all Data Scientist job postings in Kansas City, Missouri, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Data Analyst job postings in Kansas City, Missouri, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Done.\n",
      "Scraping all Data Engineer job postings in Kansas City, Missouri, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Page 34 complete.\n",
      "Page 35 complete.\n",
      "Done.\n",
      "Scraping all Machine Learning Engineer job postings in Kansas City, Missouri, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Done.\n",
      "Scraping all Business Intelligence Analyst job postings in Kansas City, Missouri, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Done.\n",
      "1508 jobs collected from Kansas City, Missouri, United States\n",
      "0 duplicate jobs found and removed.\n",
      "1508 jobs pushed to raw.jobs\n",
      "Scraping all Data Scientist job postings in Seattle, Washington, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Done.\n",
      "Scraping all Data Analyst job postings in Seattle, Washington, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Done.\n",
      "Scraping all Data Engineer job postings in Seattle, Washington, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Done.\n",
      "Scraping all Machine Learning Engineer job postings in Seattle, Washington, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Done.\n",
      "Scraping all Business Intelligence Analyst job postings in Seattle, Washington, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Done.\n",
      "1297 jobs collected from Seattle, Washington, United States\n",
      "0 duplicate jobs found and removed.\n",
      "1297 jobs pushed to raw.jobs\n",
      "Scraping all Data Scientist job postings in Washington, District of Columbia, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Page 34 complete.\n",
      "Done.\n",
      "Scraping all Data Analyst job postings in Washington, District of Columbia, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Page 34 complete.\n",
      "Page 35 complete.\n",
      "Page 36 complete.\n",
      "Done.\n",
      "Scraping all Data Engineer job postings in Washington, District of Columbia, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Machine Learning Engineer job postings in Washington, District of Columbia, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Business Intelligence Analyst job postings in Washington, District of Columbia, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Done.\n",
      "1556 jobs collected from Washington, District of Columbia, United States\n",
      "0 duplicate jobs found and removed.\n",
      "1556 jobs pushed to raw.jobs\n",
      "Scraping all Data Scientist job postings in Denver, CO, Colorado, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Data Analyst job postings in Denver, CO, Colorado, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Done.\n",
      "Scraping all Data Engineer job postings in Denver, CO, Colorado, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Page 34 complete.\n",
      "Page 35 complete.\n",
      "Done.\n",
      "Scraping all Machine Learning Engineer job postings in Denver, CO, Colorado, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Page 26 complete.\n",
      "Page 27 complete.\n",
      "Page 28 complete.\n",
      "Page 29 complete.\n",
      "Page 30 complete.\n",
      "Page 31 complete.\n",
      "Page 32 complete.\n",
      "Page 33 complete.\n",
      "Done.\n",
      "Scraping all Business Intelligence Analyst job postings in Denver, CO, Colorado, United States\n",
      "Page 1 complete.\n",
      "Page 2 complete.\n",
      "Page 3 complete.\n",
      "Page 4 complete.\n",
      "Page 5 complete.\n",
      "Page 6 complete.\n",
      "Page 7 complete.\n",
      "Page 8 complete.\n",
      "Page 9 complete.\n",
      "Page 10 complete.\n",
      "Page 11 complete.\n",
      "Page 12 complete.\n",
      "Page 13 complete.\n",
      "Page 14 complete.\n",
      "Page 15 complete.\n",
      "Page 16 complete.\n",
      "Page 17 complete.\n",
      "Page 18 complete.\n",
      "Page 19 complete.\n",
      "Page 20 complete.\n",
      "Page 21 complete.\n",
      "Page 22 complete.\n",
      "Page 23 complete.\n",
      "Page 24 complete.\n",
      "Page 25 complete.\n",
      "Done.\n",
      "1422 jobs collected from Denver, CO, Colorado, United States\n",
      "0 duplicate jobs found and removed.\n",
      "1422 jobs pushed to raw.jobs\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main\n",
    "'''\n",
    "batch_etl(search_terms, locations, schema, table, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter password: ········\n"
     ]
    }
   ],
   "source": [
    "sql_query = 'SELECT * FROM {}.{}'.format(schema, table)\n",
    "test = query_database(sql_query, username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80662, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>benefits</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>description</th>\n",
       "      <th>items</th>\n",
       "      <th>location</th>\n",
       "      <th>posted_at</th>\n",
       "      <th>qualifications</th>\n",
       "      <th>responsibilities</th>\n",
       "      <th>salary</th>\n",
       "      <th>schedule_type</th>\n",
       "      <th>search_term</th>\n",
       "      <th>title</th>\n",
       "      <th>via</th>\n",
       "      <th>work_from_home</th>\n",
       "      <th>date_scraped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCwgUHJvZH...</td>\n",
       "      <td>In addition to salary, you will also be eligib...</td>\n",
       "      <td>Etsy</td>\n",
       "      <td>2023-06-05</td>\n",
       "      <td>Company Description\\n\\nEtsy is the global mark...</td>\n",
       "      <td>None</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>2+ years of experience as a data scientist or ...</td>\n",
       "      <td>Data scientists at Etsy use rigorous methods t...</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>SmartRecruiters Job Search</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAtIEFkcy...</td>\n",
       "      <td>None</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Position Summary...\\n\\nWhat you'll do...\\n\\nww...</td>\n",
       "      <td>None</td>\n",
       "      <td>Hoboken, NJ</td>\n",
       "      <td>None</td>\n",
       "      <td>You’re proficient in modeling algorithms and p...</td>\n",
       "      <td>They lead communications and interactions with...</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist - Ads Measurement - Hoboken, NJ</td>\n",
       "      <td>Walmart Careers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBTY2llbnRpc3...</td>\n",
       "      <td>None</td>\n",
       "      <td>Glocomms</td>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>A Fortune 100 financial services provider is s...</td>\n",
       "      <td>None</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>3+ years of experience using Python, SQL, and ...</td>\n",
       "      <td>Data Scientist, you will be responsible for de...</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Glocomms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAoUmVtb3...</td>\n",
       "      <td>We are offering a market competitive salary of...</td>\n",
       "      <td>Transfix</td>\n",
       "      <td>NaT</td>\n",
       "      <td>About Transfix:\\n\\nTransfix, named to Forbes’ ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>None</td>\n",
       "      <td>You have 1+ years of working experience manipu...</td>\n",
       "      <td>In this role, you’ll learn about managing your...</td>\n",
       "      <td>89,250–150,500 a year</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist (Remote)</td>\n",
       "      <td>Built In NYC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCwgTWFya2...</td>\n",
       "      <td>Competitive compensation\\nFully remote work en...</td>\n",
       "      <td>Obviously</td>\n",
       "      <td>NaT</td>\n",
       "      <td>What We’re Looking For\\n\\nYou don’t take any d...</td>\n",
       "      <td>None</td>\n",
       "      <td>Anywhere</td>\n",
       "      <td>None</td>\n",
       "      <td>You understand the data needs of marketers and...</td>\n",
       "      <td>You’re ready to go in front of a group of anal...</td>\n",
       "      <td>None</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist, Marketing Analytics</td>\n",
       "      <td>Startup Jobs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              job_id  \\\n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCwgUHJvZH...   \n",
       "1  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAtIEFkcy...   \n",
       "2  eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBTY2llbnRpc3...   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAoUmVtb3...   \n",
       "4  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCwgTWFya2...   \n",
       "\n",
       "                                            benefits company_name date_posted  \\\n",
       "0  In addition to salary, you will also be eligib...         Etsy  2023-06-05   \n",
       "1                                               None      Walmart         NaT   \n",
       "2                                               None     Glocomms  2023-06-09   \n",
       "3  We are offering a market competitive salary of...     Transfix         NaT   \n",
       "4  Competitive compensation\\nFully remote work en...    Obviously         NaT   \n",
       "\n",
       "                                         description items           location  \\\n",
       "0  Company Description\\n\\nEtsy is the global mark...  None    Brooklyn, NY      \n",
       "1  Position Summary...\\n\\nWhat you'll do...\\n\\nww...  None     Hoboken, NJ      \n",
       "2  A Fortune 100 financial services provider is s...  None    New York, NY      \n",
       "3  About Transfix:\\n\\nTransfix, named to Forbes’ ...  None          Anywhere    \n",
       "4  What We’re Looking For\\n\\nYou don’t take any d...  None          Anywhere    \n",
       "\n",
       "    posted_at                                     qualifications  \\\n",
       "0  6 days ago  2+ years of experience as a data scientist or ...   \n",
       "1        None  You’re proficient in modeling algorithms and p...   \n",
       "2  2 days ago  3+ years of experience using Python, SQL, and ...   \n",
       "3        None  You have 1+ years of working experience manipu...   \n",
       "4        None  You understand the data needs of marketers and...   \n",
       "\n",
       "                                    responsibilities                 salary  \\\n",
       "0  Data scientists at Etsy use rigorous methods t...                   None   \n",
       "1  They lead communications and interactions with...                   None   \n",
       "2  Data Scientist, you will be responsible for de...                   None   \n",
       "3  In this role, you’ll learn about managing your...  89,250–150,500 a year   \n",
       "4  You’re ready to go in front of a group of anal...                   None   \n",
       "\n",
       "  schedule_type     search_term  \\\n",
       "0     Full-time  Data Scientist   \n",
       "1     Full-time  Data Scientist   \n",
       "2     Full-time  Data Scientist   \n",
       "3     Full-time  Data Scientist   \n",
       "4     Full-time  Data Scientist   \n",
       "\n",
       "                                            title  \\\n",
       "0               Data Scientist, Product Analytics   \n",
       "1  Data Scientist - Ads Measurement - Hoboken, NJ   \n",
       "2                           Senior Data Scientist   \n",
       "3                         Data Scientist (Remote)   \n",
       "4             Data Scientist, Marketing Analytics   \n",
       "\n",
       "                           via  work_from_home date_scraped  \n",
       "0   SmartRecruiters Job Search             NaN   2023-06-11  \n",
       "1              Walmart Careers             NaN   2023-06-11  \n",
       "2                     Glocomms             NaN   2023-06-11  \n",
       "3                 Built In NYC             1.0   2023-06-11  \n",
       "4                 Startup Jobs             1.0   2023-06-11  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
